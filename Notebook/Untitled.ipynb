{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08c1b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34ac3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/home/leehu/project/brain2image/brain2image/pretrains/ldm/label2img/config.yaml'\n",
    "\n",
    "config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4efb494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model.params.first_stage_config.params.ddconfig.ch_mult "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712b71d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.70 GiB total capacity; 1.97 GiB already allocated; 3.56 MiB free; 1.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/NFS/Users/hwlee/checkpoint.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain2image/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/brain2image/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/brain2image/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/brain2image/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/brain2image/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/brain2image/lib/python3.9/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_UntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain2image/lib/python3.9/site-packages/torch/_utils.py:78\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_UntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.70 GiB total capacity; 1.97 GiB already allocated; 3.56 MiB free; 1.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load('/NFS/Users/hwlee/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3cb7cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/DATA1/NSD/data_ae'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['config'].data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb8c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load('/home/leehu/project/brain2image/brain2image/pretrains/ldm/label2img/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "692d2fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['state_dict']['model.diffusion_model.time_embed.0.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "834cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        if config == '__is_first_stage__':\n",
    "            return None\n",
    "        elif config == \"__is_unconditional__\":\n",
    "            return None\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f4e3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.params.unet_config.params.use_time_cond = True # true\n",
    "config.model.params.unet_config.params.global_pool = False # false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45e83c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 708.32 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "model = instantiate_from_config(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5683d1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDiffusion(\n",
       "  (model): DiffusionWrapper(\n",
       "    (diffusion_model): UNetModel(\n",
       "      (time_embed): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (time_embed_condtion): Sequential(\n",
       "        (0): Conv1d(77, 38, kernel_size=(1,), stride=(1,))\n",
       "        (1): Conv1d(38, 1, kernel_size=(1,), stride=(1,))\n",
       "        (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "      (input_blocks): ModuleList(\n",
       "        (0): TimestepEmbedSequential(\n",
       "          (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (4): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (6): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (7): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(512, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(512, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (8): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (9): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (10): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(768, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(768, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (11): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (middle_block): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1024, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1024, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_blocks): ModuleList(\n",
       "        (0): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2048, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2048, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2048, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Upsample(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2048, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2048, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2048, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (4): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1280, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Upsample(\n",
       "            (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (6): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (7): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (8): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Upsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (9): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (10): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (11): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (first_stage_model): VQModelInterface(\n",
       "    (encoder): Encoder(\n",
       "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (down): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "      )\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(512, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (up): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (loss): Identity()\n",
       "    (quantize): VectorQuantizer(\n",
       "      (embedding): Embedding(8192, 3)\n",
       "    )\n",
       "    (quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (post_quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (cond_stage_model): ClassEmbedder(\n",
       "    (embedding): Embedding(1001, 512)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da301ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from eval_metrics import get_similarity_metric\n",
    "from dataloader import NSD_Dataset\n",
    "from ldm.ldm_for_fmri import fLDM\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from config import *\n",
    "import wandb\n",
    "import datetime\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "def to_image(img):\n",
    "    if img.shape[-1] != 3:\n",
    "        img = rearrange(img, 'c h w -> h w c')\n",
    "    img = 255. * img\n",
    "    return Image.fromarray(img.astype(np.uint8))\n",
    "\n",
    "def channel_last(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        return img\n",
    "    return rearrange(img, 'c h w -> h w c')\n",
    "\n",
    "def normalize(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = img * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "\n",
    "def wandb_init(config):\n",
    "    wandb.init( project=\"mind-vis\",\n",
    "                group='eval',\n",
    "                anonymous=\"allow\",\n",
    "                config=config,\n",
    "                reinit=True)\n",
    "\n",
    "def get_eval_metric(samples, avg=True):\n",
    "    metric_list = ['mse', 'pcc', 'ssim', 'psm']\n",
    "    res_list = []\n",
    "    \n",
    "    gt_images = [img[0] for img in samples]\n",
    "    gt_images = rearrange(np.stack(gt_images), 'n c h w -> n h w c')\n",
    "    samples_to_run = np.arange(1, len(samples[0])) if avg else [1]\n",
    "    for m in metric_list:\n",
    "        res_part = []\n",
    "        for s in samples_to_run:\n",
    "            pred_images = [img[s] for img in samples]\n",
    "            pred_images = rearrange(np.stack(pred_images), 'n c h w -> n h w c')\n",
    "            res = get_similarity_metric(pred_images, gt_images, method='pair-wise', metric_name=m)\n",
    "            res_part.append(np.mean(res))\n",
    "        res_list.append(np.mean(res_part))     \n",
    "    res_part = []\n",
    "    for s in samples_to_run:\n",
    "        pred_images = [img[s] for img in samples]\n",
    "        pred_images = rearrange(np.stack(pred_images), 'n c h w -> n h w c')\n",
    "        res = get_similarity_metric(pred_images, gt_images, 'class', None, \n",
    "                        n_way=50, num_trials=1000, top_k=1, device='cuda')\n",
    "        res_part.append(np.mean(res))\n",
    "    res_list.append(np.mean(res_part))\n",
    "    res_list.append(np.max(res_part))\n",
    "    metric_list.append('top-1-class')\n",
    "    metric_list.append('top-1-class (max)')\n",
    "    return res_list, metric_list\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Double Conditioning LDM Finetuning', add_help=False)\n",
    "    # project parameters\n",
    "    parser.add_argument('--root', type=str, default='.')\n",
    "    parser.add_argument('--dataset', type=str, default='GOD')\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b52fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args_parser()\n",
    "args = args.parse_args()\n",
    "root = args.root\n",
    "target = args.dataset\n",
    "#model_path = os.path.join(root, 'pretrains', f'{target}', 'finetuned.pth')\n",
    "model_path = os.path.join('/home/leehu/project/brain2image/brain2image/results/generation/09/checkpoint.pth')\n",
    "sd = torch.load(model_path, map_location='cpu')\n",
    "config = sd['config']\n",
    "\n",
    "seed = config.seed\n",
    "\n",
    "# Python의 random 시드 설정\n",
    "random.seed(seed)\n",
    "\n",
    "# Numpy 시드 설정\n",
    "np.random.seed(seed)\n",
    "\n",
    "# PyTorch 시드 설정\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# update paths\n",
    "config.root_path = root\n",
    "# config.kam_path = os.path.join(root, 'data/Kamitani/npz')\n",
    "# config.bold5000_path = os.path.join(root, 'data/BOLD5000')\n",
    "config.pretrain_mbm_path = os.path.join('/NFS/Users/hwlee/checkpoint.pth')\n",
    "config.pretrain_gm_path = os.path.join(root, 'pretrains/ldm/label2img')\n",
    "print(config.__dict__)\n",
    "\n",
    "output_path = os.path.join(config.root_path, 'results', 'eval',  \n",
    "                '%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "img_transform_test = transforms.Compose([\n",
    "    normalize, transforms.Resize((256, 256)), \n",
    "    channel_last\n",
    "])\n",
    "\n",
    "# if target == 'GOD':\n",
    "#     _, dataset_test = create_Kamitani_dataset(config.kam_path, config.roi, config.patch_size, \n",
    "#             fmri_transform=torch.FloatTensor, image_transform=img_transform_test, \n",
    "#             subjects=config.kam_subs, test_category=config.test_category)\n",
    "# elif target == 'BOLD5000':\n",
    "#     _, dataset_test = create_BOLD5000_dataset(config.bold5000_path, config.patch_size, \n",
    "#             fmri_transform=torch.FloatTensor, image_transform=img_transform_test, \n",
    "#             subjects=config.bold5000_subs)\n",
    "# else:\n",
    "#     raise NotImplementedError\n",
    "\n",
    "dataset_test = NSD_Dataset(data_dir=config.data_dir, subject_num=config.subject_num, type='val')\n",
    "\n",
    "num_voxels = dataset_test.num_voxels\n",
    "print(len(dataset_test))\n",
    "# prepare pretrained mae \n",
    "pretrain_mbm_metafile = torch.load(config.pretrain_mbm_path, map_location='cpu')\n",
    "# create generateive model\n",
    "generative_model = fLDM(pretrain_mbm_metafile, num_voxels,\n",
    "            device=device, pretrain_root=config.pretrain_gm_path, logger=config.logger,\n",
    "            ddim_steps=config.ddim_steps, global_pool=config.global_pool, use_time_cond=config.use_time_cond)\n",
    "generative_model.model.load_state_dict(sd['model_state_dict'])\n",
    "print('load ldm successfully')\n",
    "state = sd['state']\n",
    "grid, samples = generative_model.generate(dataset_test, config.num_samples, \n",
    "            config.ddim_steps, config.HW, limit=None, state=state) # generate 10 instances\n",
    "grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "grid_imgs.save(os.path.join(output_path,f'./samples_test.png'))\n",
    "\n",
    "wandb_init(config)\n",
    "wandb.log({f'summary/samples_test': wandb.Image(grid_imgs)})\n",
    "metric, metric_list = get_eval_metric(samples, avg=True)\n",
    "metric_dict = {f'summary/pair-wise_{k}':v for k, v in zip(metric_list[:-2], metric[:-2])}\n",
    "metric_dict[f'summary/{metric_list[-2]}'] = metric[-2]\n",
    "metric_dict[f'summary/{metric_list[-1]}'] = metric[-1]\n",
    "print(metric_dict)\n",
    "wandb.log(metric_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72bbf317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "a = torch.randn([1,3,256,256])\n",
    "b = torch.randn([1,3,256,256])\n",
    "sample = torch.cat([a, b], dim=0)\n",
    "all_samples.append(sample)\n",
    "\n",
    "a = torch.randn([1,3,256,256])\n",
    "b = torch.randn([1,3,256,256])\n",
    "sample = torch.cat([a, b], dim=0)\n",
    "all_samples.append(sample)\n",
    "\n",
    "a = torch.randn([1,3,256,256])\n",
    "b = torch.randn([1,3,256,256])\n",
    "sample = torch.cat([a, b], dim=0)\n",
    "all_samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "693a1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.stack(all_samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7784fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = rearrange(grid, 'n b c h w -> (n b) c h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c7e1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(grid, nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67d5d10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1292, 1034])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
